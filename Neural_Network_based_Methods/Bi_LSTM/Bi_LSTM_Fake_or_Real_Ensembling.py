import numpy as np
import keras
from keras import Model
from keras.optimizers import Adam
from keras.layers import Input
from sklearn.model_selection import train_test_split

from Bi_LSTM.Bi_LSTM_Create_Model import create_model
import pickle


def ensemble(models, model_input):
    Models_output=[ model(model_input) for model in models]
    Avg = keras.layers.average(Models_output)

    modelEnsemble = Model(inputs=model_input, outputs=Avg, name='ensemble')
    modelEnsemble.summary()
    modelEnsemble.compile(Adam(lr=.0001), loss='binary_crossentropy', metrics=['accuracy'])
    return modelEnsemble


############################################################################
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer

vocabulary_size = 400000
time_step=300

dataVal_Fake_Real=pd.read_csv('fake_or_real_news.csv')

texts=[]
texts=dataVal_Fake_Real['text']#####################################
label=dataVal_Fake_Real['label']

from DataPrep.Clean_Texts import clean_text
X=texts.map(lambda x: clean_text(x))
#print(X)
#label=label.astype(int).values
labelEncoder=LabelEncoder()
encoded_label=labelEncoder.fit_transform(label)
y=np.reshape(encoded_label,(-1,1))


#X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)
training_size=int(0.8*X.shape[0])
X_train=X[:training_size]
y_train=y[:training_size]
X_test=X[training_size:]
y_test=y[training_size:]


#Tokenizing texts
tokenizer = Tokenizer(num_words= vocabulary_size)
tokenizer.fit_on_texts(X_train)
sequences_train= tokenizer.texts_to_sequences(X_train)
X_train = sequence.pad_sequences(sequences_train, maxlen=time_step,padding='post')

print(len(tokenizer.word_index))


vocab_size=len(tokenizer.word_index)+1

#Reading Glove
f = open('glove.6B.100d.txt',encoding='utf-8')
embeddings={}
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings[word] = coefs
f.close()

print('Total %s word vectors.' % len(embeddings))

embedding_size=100

# create a weight matrix for words in training docs
embedding_matrix = np.zeros((vocab_size, embedding_size))
for word, i in tokenizer.word_index.items():
	embedding_vector = embeddings.get(word)
	if embedding_vector is not None:
		embedding_matrix[i] = embedding_vector

print(embedding_matrix.shape[0],embedding_matrix.shape[1])


sequences_test= tokenizer.texts_to_sequences(X_test)
X_test = sequence.pad_sequences(sequences_test, maxlen=time_step,padding='post')
vocab_size=embedding_matrix.shape[0]
###############################################################################################



model_1 = create_model(vocabulary_size=embedding_matrix.shape[0],embedding_size=100,embedding_matrix=embedding_matrix)
model_2 = create_model(vocabulary_size=embedding_matrix.shape[0],embedding_size=100,embedding_matrix=embedding_matrix)
model_3 = create_model(vocabulary_size=embedding_matrix.shape[0],embedding_size=100,embedding_matrix=embedding_matrix)
model_4 = create_model(vocabulary_size=embedding_matrix.shape[0],embedding_size=100,embedding_matrix=embedding_matrix)
model_5 = create_model(vocabulary_size=embedding_matrix.shape[0],embedding_size=100,embedding_matrix=embedding_matrix)

models = []

# Load weights
print("Load Weights")

model_1.load_weights('Models/Bi_LSTM/Cross_Validation/FR/Model_cv_bi_lstm_FR_1_kfold_1.h5')
model_1.name = 'model_1'
models.append(model_1)

model_2.load_weights('Models/Bi_LSTM/Cross_Validation/FR/Model_cv_bi_lstm_FR_1_kfold_2.h5')
model_2.name = 'model_2'
models.append(model_2)

model_3.load_weights('Models/Bi_LSTM/Cross_Validation/FR/Model_cv_bi_lstm_FR_1_kfold_3.h5')
model_3.name = 'model_3'
models.append(model_3)

model_4.load_weights('Models/Bi_LSTM/Cross_Validation/FR/Model_cv_bi_lstm_FR_1_kfold_4.h5')
model_4.name = 'model_4'
models.append(model_4)

model_5.load_weights('Models/Bi_LSTM/Cross_Validation/FR/Model_cv_bi_lstm_FR_1_kfold_5.h5')
model_5.name = 'model_5'
models.append(model_5)

print("Model input: "+str(models[0].input_shape[1:]))
model_input = Input(shape=models[0].input_shape[1:])
ensemble_model = ensemble(models, model_input)



############################### FR Test  ##############################################

scores = ensemble_model.evaluate(X_test, y_test, verbose=0)
print("%s: %.2f%%" % (ensemble_model.metrics_names[1], scores[1]))

from sklearn.metrics import precision_recall_fscore_support, classification_report

y_pred = ensemble_model.predict(X_test)
print(len(X_test)+len(X_train))

y_pred_2=np.reshape(y_pred,(len(y_pred)))
y_test_2=np.reshape(y_test,(len(y_test)))
y_pred_2=(y_pred_2>0.5) # Boolean
print('Classification Report: '+classification_report(y_test_2, y_pred_2))

print("Saving Model...")
model_name = 'Models/Ensemble/5-Fold/Model_ensemble_bi_lstm_FR_1.h5'
ensemble_model.save(model_name)
#################################################

from sklearn.metrics import precision_recall_fscore_support, classification_report
y_pred=model_1.predict(X_test)
y_pred_2=(y_pred>0.5)
print('Classification Report: '+classification_report(y_test, y_pred_2))

import pickle
with open('Predictions/pickle_Pred_bi_LSTM_1_FR.pickle','wb') as f:
    pickle.dump((y_test,y_pred_2),f)


