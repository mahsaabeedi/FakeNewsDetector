from keras.models import Sequential,load_model
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
import pickle
from sklearn.model_selection import KFold
from Bi_LSTM.Bi_LSTM_Create_Model import create_model,callback,plot_loss_accu
from sklearn.preprocessing import LabelEncoder
import gc
import keras.backend as K

###########################################################################
vocabulary_size = 400000

time_step=300

dataset=pd.read_csv('train.csv')
texts=[]
#texts=dataset['Statement'].astype(str).values.tolist()

texts=dataset['Statement']
from DataPrep.Clean_Texts import clean_text
texts=texts.map(lambda x: clean_text(x))

label=dataset['Label'].astype(int).values.tolist()
labelEncoder=LabelEncoder()
encoded_label=labelEncoder.fit_transform(label)
y=np.reshape(encoded_label,(-1,1))

tokenizer_train=Tokenizer(num_words=vocabulary_size)
tokenizer_train.fit_on_texts(texts)
encoded_train=tokenizer_train.texts_to_sequences(texts=texts)
#print(encoded_docs)
vocab_size_train = len(tokenizer_train.word_index) + 1
print(vocab_size_train)

X = sequence.pad_sequences(encoded_train, maxlen=time_step,padding='post')


f = open('glove.6B.100d.txt',encoding='utf-8')
embeddings_train={}
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_train[word] = coefs
f.close()

print('Total %s word vectors.' % len(embeddings_train))

embedding_size=100

# create a weight matrix for words in training docs
embedding_matrix = np.zeros((vocab_size_train, embedding_size))
for word, i in tokenizer_train.word_index.items():
    embedding_vector_train = embeddings_train.get(word)
    if embedding_vector_train is not None:
        embedding_matrix[i] = embedding_vector_train


dataset=pd.read_csv('test.csv')
statement=dataset.iloc[:,2:3].values
#statement=statement.lower()
texts=[]

texts=dataset['Statement']
from DataPrep.Clean_Texts import clean_text
texts=texts.map(lambda x: clean_text(x))

label=dataset['Label'].astype(int).values.tolist()
labelEncoder=LabelEncoder()
encoded_label=labelEncoder.fit_transform(label)
y_test=np.reshape(encoded_label,(-1,1))

encoded_test=tokenizer_train.texts_to_sequences(texts=texts)
X_test = sequence.pad_sequences(encoded_test, maxlen=time_step, padding='post')

vocab_size=embedding_matrix.shape[0]
##########################################################################################


kfold = KFold(n_splits=5, shuffle=True, random_state=42)##################
cvscores_Liar=[]
classfication_report=[]

Fold = 1
for train, val in kfold.split(X, y):
    gc.collect()
    K.clear_session()
    print('Fold: ', Fold)

    X_train = X[train]
    X_val = X[val]

    y_train = y[train]
    y_val = y[val]

    print("Initializing Callback :/...")

    model_name = 'Models/Bi_LSTM/Cross_Validation/Callbacks/Liar/Model_cv_bi_lstm_Liar_1_Callbacks_kfold_'+str(Fold)+'.h5'
    cb = callback(model_name=model_name) #####################################################################

    # create model
    print("Creating and Fitting Model...")
    model = create_model(vocabulary_size=vocab_size,embedding_size=embedding_size,embedding_matrix=embedding_matrix)

    history=model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=128,shuffle=True,callbacks=cb)##############
    ######################### #callback chilo

    # Save each fold model
    print("Saving Model...")
    model_name = 'Models/Bi_LSTM/Cross_Validation/Liar/Model_cv_bi_lstm_Liar_1_kfold_' + str(Fold) + '.h5'########################################3
    model.save(model_name)#################################################
    '''
    model = load_model('Models/Bi_LSTM/Cross_Validation/Liar/Model_cv_bi_lstm_Liar_1_kfold_' + str(Fold) + '.h5')
    model.name='Model_bi_lstm_Liar_1.h5'
    '''

    # evaluate the model
    print("Evaluating Model...")
    ##########################################
    scores = model.evaluate(X_test, y_test, verbose=0)
    print("Eval with Fake or Real %s: %.2f%%" % (model.metrics_names[1], scores[1]))
    cvscores_Liar.append(scores[1])

    from sklearn.metrics import precision_recall_fscore_support, classification_report

    y_pred = model.predict_classes(X_test)
    classfication_report.append(classification_report(y_test, y_pred))
    #print('Classification report:\n', classification_report(y_test, y_pred))
    # print('Classification report:\n',precision_recall_fscore_support(y_test,y_pred))
    # print(y_pred)

    '''#######################################################
    ########### Saving Graph ####################
    print("Saving graph...")

    plot_loss_accu(history,'Graphs/Train_Val_Loss_Fold_'+str(Fold)+'.png','Graphs/Train_Val_Acc_Fold_'+str(Fold)+'.png')
    #######################################################'''

    Fold = Fold + 1

print("Accuracy list of Fake or Real: ",cvscores_Liar)
print("%s: %.2f%%" % ("Mean Accuracy of Fake or Real: ", np.mean(cvscores_Liar)))
print("%s: %.2f%%" % ("Standard Deviation of Fake or Real: +/-", np.std(cvscores_Liar)))


print('Classfication Report:')
for cr in classfication_report:
    print(cr)
