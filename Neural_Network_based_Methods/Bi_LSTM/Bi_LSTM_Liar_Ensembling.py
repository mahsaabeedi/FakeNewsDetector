import numpy as np
import keras
from keras import Model
from keras.optimizers import Adam
from keras.layers import Input
from sklearn.model_selection import train_test_split

from Bi_LSTM.Bi_LSTM_Create_Model import create_model
import pickle


def ensemble(models, model_input):
    Models_output=[ model(model_input) for model in models]
    Avg = keras.layers.average(Models_output)

    modelEnsemble = Model(inputs=model_input, outputs=Avg, name='ensemble')
    modelEnsemble.summary()
    modelEnsemble.compile(Adam(lr=.0001), loss='binary_crossentropy', metrics=['accuracy'])
    return modelEnsemble


############################################################################
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer

vocabulary_size = 400000
time_step=300

dataset=pd.read_csv('train.csv')
texts=[]
texts=dataset['Statement']
from DataPrep.Clean_Texts import clean_text
texts=texts.map(lambda x: clean_text(x))

label=dataset['Label'].astype(int).values.tolist()
labelEncoder=LabelEncoder()
encoded_label=labelEncoder.fit_transform(label)
y=np.reshape(encoded_label,(-1,1))

tokenizer_train=Tokenizer(num_words=vocabulary_size)
tokenizer_train.fit_on_texts(texts)
encoded_train=tokenizer_train.texts_to_sequences(texts=texts)
vocab_size_train = len(tokenizer_train.word_index) + 1
print(vocab_size_train)
X = sequence.pad_sequences(encoded_train, maxlen=time_step,padding='post')


f = open('glove.6B.100d.txt',encoding='utf-8')
embeddings_train={}
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_train[word] = coefs
f.close()
print('Total %s word vectors.' % len(embeddings_train))

embedding_size=100
embedding_matrix = np.zeros((vocab_size_train, embedding_size))
for word, i in tokenizer_train.word_index.items():
    embedding_vector_train = embeddings_train.get(word)
    if embedding_vector_train is not None:
        embedding_matrix[i] = embedding_vector_train


dataset=pd.read_csv('test.csv')
texts=[]
texts=dataset['Statement']
texts=texts.map(lambda x: clean_text(x))

label=dataset['Label'].astype(int).values.tolist()
labelEncoder=LabelEncoder()
encoded_label=labelEncoder.fit_transform(label)
y_test=np.reshape(encoded_label,(-1,1))

encoded_test=tokenizer_train.texts_to_sequences(texts=texts)
X_test = sequence.pad_sequences(encoded_test, maxlen=time_step, padding='post')
vocab_size=embedding_matrix.shape[0]
###############################################################################################



model_1 = create_model(vocabulary_size=embedding_matrix.shape[0],embedding_size=100,embedding_matrix=embedding_matrix)
model_2 = create_model(vocabulary_size=embedding_matrix.shape[0],embedding_size=100,embedding_matrix=embedding_matrix)
model_3 = create_model(vocabulary_size=embedding_matrix.shape[0],embedding_size=100,embedding_matrix=embedding_matrix)
model_4 = create_model(vocabulary_size=embedding_matrix.shape[0],embedding_size=100,embedding_matrix=embedding_matrix)
model_5 = create_model(vocabulary_size=embedding_matrix.shape[0],embedding_size=100,embedding_matrix=embedding_matrix)

models = []

# Load weights
print("Load Weights")

model_1.load_weights('Models/Bi_LSTM/Cross_Validation/Liar/Model_cv_bi_lstm_Liar_1_kfold_1.h5')
model_1.name = 'model_1'
models.append(model_1)

model_2.load_weights('Models/Bi_LSTM/Cross_Validation/Liar/Model_cv_bi_lstm_Liar_1_kfold_2.h5')
model_2.name = 'model_2'
models.append(model_2)

model_3.load_weights('Models/Bi_LSTM/Cross_Validation/Liar/Model_cv_bi_lstm_Liar_1_kfold_3.h5')
model_3.name = 'model_3'
models.append(model_3)

model_4.load_weights('Models/Bi_LSTM/Cross_Validation/Liar/Model_cv_bi_lstm_Liar_1_kfold_4.h5')
model_4.name = 'model_4'
models.append(model_4)

model_5.load_weights('Models/Bi_LSTM/Cross_Validation/Liar/Model_cv_bi_lstm_Liar_1_kfold_5.h5')
model_5.name = 'model_5'
models.append(model_5)

print("Model input: "+str(models[0].input_shape[1:]))
model_input = Input(shape=models[0].input_shape[1:])
ensemble_model = ensemble(models, model_input)



############################### Liar Test  ##############################################

scores = ensemble_model.evaluate(X_test, y_test, verbose=0)
print("%s: %.2f%%" % (ensemble_model.metrics_names[1], scores[1]))

from sklearn.metrics import precision_recall_fscore_support, classification_report

y_pred = ensemble_model.predict(X_test)

y_pred_2=np.reshape(y_pred,(len(y_pred)))
y_test_2=np.reshape(y_test,(len(y_test)))
y_pred_2=(y_pred_2>0.5) # Boolean
print('Classification Report: '+classification_report(y_test_2, y_pred_2))

print("Saving Model...")
model_name = 'Models/Ensemble/5-Fold/Model_ensemble_bi_lstm_Liar_1.h5'
ensemble_model.save(model_name)#################################################


from sklearn.metrics import precision_recall_fscore_support, classification_report
y_pred=model_1.predict(X_test)
y_pred_2=(y_pred>0.5)
print('Classification Report: '+classification_report(y_test, y_pred_2))

import pickle
with open('Predictions/pickle_Pred_bi_LSTM_1_Liar.pickle','wb') as f:
    pickle.dump((y_test,y_pred_2),f)
